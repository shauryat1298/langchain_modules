{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197c6191",
   "metadata": {},
   "source": [
    "### Retriever and Chain with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75f3b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d1c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba3e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"lec_6.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "db = FAISS.from_documents(split_docs[:10], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6af6da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3f470646-df81-45f9-a5a0-2c18d4abd515', metadata={'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2024-03-19T21:25:11-05:00', 'author': 'Shaurya Tripathi', 'moddate': '2024-03-19T21:25:11-05:00', 'title': 'Microsoft PowerPoint - 3_Self_Attention_Transformer', 'source': 'lec_6.pdf', 'total_pages': 56, 'page': 8, 'page_label': '9'}, page_content='3/19/2024\\n9\\nhttps://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture09-transformers.pdfAttention is a general Deep Learning techniqueGiven a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values,  dependent on the query .Intuition:•The weighted sum is a selective summary of the  information contained in the values, where the query  determines which values to focus on.•Attention is a way to obtain a fixed-size representation  of an arbitrary set of representations (the values),  dependent on some other representation (the query).\\nhttps://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture09-transformers.pdf\\n17\\n18'),\n",
       " Document(id='fd1537dc-2397-4872-99da-be456df62f9d', metadata={'producer': 'Microsoft: Print To PDF', 'creator': 'PyPDF', 'creationdate': '2024-03-19T21:25:11-05:00', 'author': 'Shaurya Tripathi', 'moddate': '2024-03-19T21:25:11-05:00', 'title': 'Microsoft PowerPoint - 3_Self_Attention_Transformer', 'source': 'lec_6.pdf', 'total_pages': 56, 'page': 3, 'page_label': '4'}, page_content='3/19/2024\\n4\\nAttention we’ve seen so farNow known as “additive” recurrentattention (type of encoder-decoder attention)𝒉𝟏𝒉𝟐𝒉𝟑𝒉𝟒𝑒௧ଵ𝑒௧ଶ𝑒௧ଷ𝑒௧ସ𝛼௧ଵ𝛼௧ଶ𝛼௧ଷ𝛼௧ସsoftmax𝑓att𝒔𝒕ି𝟏𝑓att𝑓att𝑓attmul + addAlignment scores𝑒௧\\u0bdc=𝑓att(𝒔𝒕ି𝟏 ,𝒉𝒊)Attention weights𝜶𝒕=  softmax(𝒆𝒕)𝒄𝒕Context vector𝒄𝒕=\\u0dcd𝛼௧\\u0bdc𝒉𝒊\\u0bdcInput𝑓att: simple feedforward network (e.g. MLP)AlignmentAttention𝒔𝒕•••Slide Credit : https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/Decoupling from RNNs•Recall: attention determines the importance of elementsto be passed forward in the model.•These weights lets the model pay attentionto the most significant parts•Objective: a more general attention mechanism not confined to RNNs •We need a modified procedure to:1. Determine weights based on context that indicate the elements to attend to2. Apply these weights to enhance attended featuresSlide Credit : https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/\\n7\\n8')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"An attention function can be what?\"\n",
    "results = db.similarity_search(query, k=2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82880d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='llama2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7410e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based on the context I give you. Think step by step\n",
    "    <context> {context} </context>\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6949a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000188CEE39BD0>, search_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chain Introduction\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10804811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question you've provided is related to the context given earlier, which discusses attention in the context of Recurrent Neural Networks (RNNs). Attention in RNNs refers to a technique used to selectively focus on certain parts of the input sequence when processing it. This is done by computing a weighted sum of the input elements, where the weights are determined by the query vector. The attention mechanism allows the model to pay more attention to the most relevant parts of the input sequence, rather than treating all elements equally.\n",
      "\n",
      "In RNNs, attention is typically used in the encoder-decoder architecture, where the encoder processes the input sequence and produces a context vector, and the decoder processes the context vector and generates the output sequence. The attention mechanism is applied to the input sequence to determine which parts are most relevant for the decoder to generate the output sequence.\n",
      "\n",
      "The issues with recurrent attention, as mentioned in the context, include scalability problems, performance degradation as the distance between words increases, parallelization limitations, and memory constraints. These issues can be addressed by decoupling attention from RNNs, which allows for a more general attention mechanism that is not confined to RNNs.\n"
     ]
    }
   ],
   "source": [
    "## Retriever chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "ans = retrieval_chain.invoke({\n",
    "    \"input\": \"What is attention in RNN\"\n",
    "})['answer']\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd0f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
